{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News_Virality.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4lGX1DosB9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvRNinTpsiCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "f6bb6a07-ec44-46d9-86ca-d760719899f5"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-alpha0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVygcIVwuSup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv(\"drive/My Drive/news_dataset.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQUW5O-QuSxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shares=dataset[\"Shares\"]  #Target\n",
        "for i in range(len(shares)):\n",
        "    temp=shares[i]\n",
        "    temp=temp.split(\" \")\n",
        "    temp=temp[0]\n",
        "    if temp[-1]==\"K\":\n",
        "        temp=float(temp[:-1])*1000\n",
        "    elif temp[-1]==\"M\":\n",
        "        temp=float(temp[:-1])*1000000\n",
        "    else:\n",
        "        temp=float(temp)\n",
        "    shares[i]=temp\n",
        "dataset[\"Shares\"]=shares\n",
        "dataset[\"Shares\"]=dataset[\"Shares\"].apply(lambda x: 1 if x>1500 else 0)  #Setting the threshold for the virality to 1000 shares\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEUk6_BdNZV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hyperparameters\n",
        "vocab_size=8000\n",
        "embedding_dim=32\n",
        "max_length=400\n",
        "trunc_type='post'\n",
        "oov_tok=\"<00V>\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSbe55fVuS1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "176712bf-083a-4b11-9773-ae0576cf5713"
      },
      "source": [
        "\n",
        "\n",
        "articles=dataset[\"Article\"]\n",
        "articles[1620]=\"Nothing\"\n",
        "\n",
        "for k in range(1864):\n",
        "  articles[k]=''.join([j for j in articles[k] if not j.isdigit()])\n",
        "dataset[\"Article\"]=articles\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBNk88x6LroM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "cf05dd02-03da-4c07-9cf1-503f6df8fd67"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    dataset[\"Article\"][i]=re.sub(\"[^a-zA-Z]\",\" \",dataset[\"Article\"][i])\n",
        "    dataset[\"Article\"][i]=dataset[\"Article\"][i].lower()\n",
        "    dataset[\"Article\"][i]=dataset[\"Article\"][i].split()\n",
        "    ps=PorterStemmer()\n",
        "    dataset[\"Article\"][i]=[ps.stem(word) for word in dataset[\"Article\"][i] if not word in set(stopwords.words(\"english\"))]\n",
        "    dataset[\"Article\"][i]=\" \".join(dataset[\"Article\"][i])\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size,oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts((dataset[\"Article\"][:]).astype(str))\n",
        "word_index = tokenizer.word_index\n",
        "sequences=tokenizer.texts_to_sequences((dataset[\"Article\"][:]).astype(str))\n",
        "padded=pad_sequences(sequences,maxlen=max_length,truncating=trunc_type,padding=\"post\")\n",
        "X=padded\n",
        "y=dataset.iloc[:,3].values\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_do8bL92uSwI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3c8b3f9-6598-491f-ddce-5baed43cab5b"
      },
      "source": [
        "#NN\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n",
        "model=tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),\n",
        "                            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "                           tf.keras.layers.Dropout(0.3),\n",
        "                            tf.keras.layers.Dense(16,activation='relu',kernel_regularizer=tf.keras.regularizers.l2()),\n",
        "                           tf.keras.layers.Dense(32,activation='relu',kernel_regularizer=tf.keras.regularizers.l2()),\n",
        "                            tf.keras.layers.Dense(1,activation='sigmoid')])\n",
        "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.RMSprop(),metrics=['accuracy'])\n",
        "num_epochs=50\n",
        "model.fit(X,\n",
        "         y,\n",
        "         epochs=num_epochs,\n",
        "         validation_data=(X_test,y_test))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1864 samples, validate on 373 samples\n",
            "Epoch 1/50\n",
            "1864/1864 [==============================] - 1s 374us/sample - loss: 0.9918 - accuracy: 0.6438 - val_loss: 0.9071 - val_accuracy: 0.6193\n",
            "Epoch 2/50\n",
            "1864/1864 [==============================] - 0s 224us/sample - loss: 0.8330 - accuracy: 0.6491 - val_loss: 0.7967 - val_accuracy: 0.6193\n",
            "Epoch 3/50\n",
            "1864/1864 [==============================] - 0s 210us/sample - loss: 0.7414 - accuracy: 0.6491 - val_loss: 0.7252 - val_accuracy: 0.6193\n",
            "Epoch 4/50\n",
            "1864/1864 [==============================] - 0s 218us/sample - loss: 0.6876 - accuracy: 0.6491 - val_loss: 0.6868 - val_accuracy: 0.6193\n",
            "Epoch 5/50\n",
            "1864/1864 [==============================] - 0s 217us/sample - loss: 0.6610 - accuracy: 0.6491 - val_loss: 0.6716 - val_accuracy: 0.6193\n",
            "Epoch 6/50\n",
            "1864/1864 [==============================] - 0s 210us/sample - loss: 0.6514 - accuracy: 0.6491 - val_loss: 0.6677 - val_accuracy: 0.6193\n",
            "Epoch 7/50\n",
            "1864/1864 [==============================] - 0s 221us/sample - loss: 0.6494 - accuracy: 0.6491 - val_loss: 0.6663 - val_accuracy: 0.6193\n",
            "Epoch 8/50\n",
            "1864/1864 [==============================] - 0s 211us/sample - loss: 0.6488 - accuracy: 0.6491 - val_loss: 0.6676 - val_accuracy: 0.6193\n",
            "Epoch 9/50\n",
            "1864/1864 [==============================] - 0s 210us/sample - loss: 0.6486 - accuracy: 0.6491 - val_loss: 0.6673 - val_accuracy: 0.6193\n",
            "Epoch 10/50\n",
            "1864/1864 [==============================] - 0s 219us/sample - loss: 0.6484 - accuracy: 0.6491 - val_loss: 0.6666 - val_accuracy: 0.6193\n",
            "Epoch 11/50\n",
            "1864/1864 [==============================] - 0s 206us/sample - loss: 0.6481 - accuracy: 0.6491 - val_loss: 0.6651 - val_accuracy: 0.6193\n",
            "Epoch 12/50\n",
            "1864/1864 [==============================] - 0s 222us/sample - loss: 0.6479 - accuracy: 0.6491 - val_loss: 0.6649 - val_accuracy: 0.6193\n",
            "Epoch 13/50\n",
            "1864/1864 [==============================] - 0s 205us/sample - loss: 0.6472 - accuracy: 0.6491 - val_loss: 0.6661 - val_accuracy: 0.6193\n",
            "Epoch 14/50\n",
            "1864/1864 [==============================] - 0s 205us/sample - loss: 0.6458 - accuracy: 0.6491 - val_loss: 0.6641 - val_accuracy: 0.6193\n",
            "Epoch 15/50\n",
            "1864/1864 [==============================] - 0s 222us/sample - loss: 0.6423 - accuracy: 0.6491 - val_loss: 0.6581 - val_accuracy: 0.6193\n",
            "Epoch 16/50\n",
            "1864/1864 [==============================] - 0s 209us/sample - loss: 0.6362 - accuracy: 0.6491 - val_loss: 0.6506 - val_accuracy: 0.6193\n",
            "Epoch 17/50\n",
            "1864/1864 [==============================] - 0s 220us/sample - loss: 0.6276 - accuracy: 0.6491 - val_loss: 0.6401 - val_accuracy: 0.6193\n",
            "Epoch 18/50\n",
            "1864/1864 [==============================] - 0s 209us/sample - loss: 0.6144 - accuracy: 0.6491 - val_loss: 0.6244 - val_accuracy: 0.6193\n",
            "Epoch 19/50\n",
            "1864/1864 [==============================] - 0s 203us/sample - loss: 0.6020 - accuracy: 0.6491 - val_loss: 0.6123 - val_accuracy: 0.6193\n",
            "Epoch 20/50\n",
            "1864/1864 [==============================] - 0s 212us/sample - loss: 0.5902 - accuracy: 0.6685 - val_loss: 0.6018 - val_accuracy: 0.6810\n",
            "Epoch 21/50\n",
            "1864/1864 [==============================] - 0s 205us/sample - loss: 0.5826 - accuracy: 0.7248 - val_loss: 0.6002 - val_accuracy: 0.6515\n",
            "Epoch 22/50\n",
            "1864/1864 [==============================] - 0s 201us/sample - loss: 0.5729 - accuracy: 0.7334 - val_loss: 0.5818 - val_accuracy: 0.7024\n",
            "Epoch 23/50\n",
            "1864/1864 [==============================] - 0s 209us/sample - loss: 0.5617 - accuracy: 0.7495 - val_loss: 0.5677 - val_accuracy: 0.7453\n",
            "Epoch 24/50\n",
            "1864/1864 [==============================] - 0s 198us/sample - loss: 0.5495 - accuracy: 0.7704 - val_loss: 0.5618 - val_accuracy: 0.7212\n",
            "Epoch 25/50\n",
            "1864/1864 [==============================] - 0s 216us/sample - loss: 0.5429 - accuracy: 0.7736 - val_loss: 0.5475 - val_accuracy: 0.7480\n",
            "Epoch 26/50\n",
            "1864/1864 [==============================] - 0s 211us/sample - loss: 0.5285 - accuracy: 0.7935 - val_loss: 0.5299 - val_accuracy: 0.7828\n",
            "Epoch 27/50\n",
            "1864/1864 [==============================] - 0s 217us/sample - loss: 0.5180 - accuracy: 0.8020 - val_loss: 0.5167 - val_accuracy: 0.7962\n",
            "Epoch 28/50\n",
            "1864/1864 [==============================] - 0s 213us/sample - loss: 0.5056 - accuracy: 0.8192 - val_loss: 0.5032 - val_accuracy: 0.8070\n",
            "Epoch 29/50\n",
            "1864/1864 [==============================] - 0s 201us/sample - loss: 0.4939 - accuracy: 0.8353 - val_loss: 0.4909 - val_accuracy: 0.8579\n",
            "Epoch 30/50\n",
            "1864/1864 [==============================] - 0s 211us/sample - loss: 0.4819 - accuracy: 0.8466 - val_loss: 0.4994 - val_accuracy: 0.7668\n",
            "Epoch 31/50\n",
            "1864/1864 [==============================] - 0s 204us/sample - loss: 0.4694 - accuracy: 0.8568 - val_loss: 0.4636 - val_accuracy: 0.8472\n",
            "Epoch 32/50\n",
            "1864/1864 [==============================] - 0s 207us/sample - loss: 0.4595 - accuracy: 0.8680 - val_loss: 0.4599 - val_accuracy: 0.8257\n",
            "Epoch 33/50\n",
            "1864/1864 [==============================] - 0s 214us/sample - loss: 0.4452 - accuracy: 0.8670 - val_loss: 0.4392 - val_accuracy: 0.8767\n",
            "Epoch 34/50\n",
            "1864/1864 [==============================] - 0s 207us/sample - loss: 0.4327 - accuracy: 0.8847 - val_loss: 0.4337 - val_accuracy: 0.8552\n",
            "Epoch 35/50\n",
            "1864/1864 [==============================] - 0s 204us/sample - loss: 0.4254 - accuracy: 0.8863 - val_loss: 0.4164 - val_accuracy: 0.8928\n",
            "Epoch 36/50\n",
            "1864/1864 [==============================] - 0s 210us/sample - loss: 0.4130 - accuracy: 0.8911 - val_loss: 0.4053 - val_accuracy: 0.9169\n",
            "Epoch 37/50\n",
            "1864/1864 [==============================] - 0s 204us/sample - loss: 0.4045 - accuracy: 0.9061 - val_loss: 0.3961 - val_accuracy: 0.9062\n",
            "Epoch 38/50\n",
            "1864/1864 [==============================] - 0s 211us/sample - loss: 0.3937 - accuracy: 0.9067 - val_loss: 0.3880 - val_accuracy: 0.9008\n",
            "Epoch 39/50\n",
            "1864/1864 [==============================] - 0s 201us/sample - loss: 0.3864 - accuracy: 0.9126 - val_loss: 0.3785 - val_accuracy: 0.9088\n",
            "Epoch 40/50\n",
            "1864/1864 [==============================] - 0s 205us/sample - loss: 0.3722 - accuracy: 0.9222 - val_loss: 0.3665 - val_accuracy: 0.9303\n",
            "Epoch 41/50\n",
            "1864/1864 [==============================] - 0s 212us/sample - loss: 0.3660 - accuracy: 0.9249 - val_loss: 0.3565 - val_accuracy: 0.9330\n",
            "Epoch 42/50\n",
            "1864/1864 [==============================] - 0s 206us/sample - loss: 0.3556 - accuracy: 0.9249 - val_loss: 0.3495 - val_accuracy: 0.9223\n",
            "Epoch 43/50\n",
            "1864/1864 [==============================] - 0s 226us/sample - loss: 0.3479 - accuracy: 0.9367 - val_loss: 0.3403 - val_accuracy: 0.9303\n",
            "Epoch 44/50\n",
            "1864/1864 [==============================] - 0s 202us/sample - loss: 0.3386 - accuracy: 0.9351 - val_loss: 0.3550 - val_accuracy: 0.9062\n",
            "Epoch 45/50\n",
            "1864/1864 [==============================] - 0s 216us/sample - loss: 0.3337 - accuracy: 0.9447 - val_loss: 0.3239 - val_accuracy: 0.9437\n",
            "Epoch 46/50\n",
            "1864/1864 [==============================] - 0s 212us/sample - loss: 0.3232 - accuracy: 0.9453 - val_loss: 0.3170 - val_accuracy: 0.9544\n",
            "Epoch 47/50\n",
            "1864/1864 [==============================] - 0s 208us/sample - loss: 0.3180 - accuracy: 0.9458 - val_loss: 0.3131 - val_accuracy: 0.9383\n",
            "Epoch 48/50\n",
            "1864/1864 [==============================] - 0s 204us/sample - loss: 0.3068 - accuracy: 0.9539 - val_loss: 0.3125 - val_accuracy: 0.9625\n",
            "Epoch 49/50\n",
            "1864/1864 [==============================] - 0s 214us/sample - loss: 0.2994 - accuracy: 0.9544 - val_loss: 0.2967 - val_accuracy: 0.9517\n",
            "Epoch 50/50\n",
            "1864/1864 [==============================] - 0s 209us/sample - loss: 0.2933 - accuracy: 0.9571 - val_loss: 0.3102 - val_accuracy: 0.9330\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa5331eab38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0KcdwrgfyIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "23bf064e-dc9c-4596-e8d6-b8f0684153a6"
      },
      "source": [
        "y_pred=model.predict(X_test)\n",
        "for i in range(len(y_pred)):\n",
        "  if y_pred[i]>=0.5:\n",
        "    y_pred[i]=1\n",
        "  else:\n",
        "    y_pred[i]=0\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "cm"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[225,   6],\n",
              "       [ 19, 123]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}